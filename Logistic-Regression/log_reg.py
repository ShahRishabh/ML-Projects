# -*- coding: utf-8 -*-
"""Q2_mfds project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1teX0B87UBw2AYt1iwy-cz1PkEOTW5wWY
"""

import numpy as np
import pandas as pd
import random
import sklearn

data = pd.read_excel('Dataset_Question2.xlsx')    #importing data

#Group the feature columns as X
features_col = ['Temperature', 'Pressure', 'Feed Flow rate',	'Coolant Flow rate',	'Inlet reactant concentration']
X = data[features_col]
X[:5]

#Define output column
Y = data['Test']

#Converting the categorical output to quantitative
Y = [1 if i == "Pass" else 0 for i in Y]

#Standardizing the features
from sklearn import preprocessing
X = preprocessing.scale(X)
X

#Test-train split
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=0)

"""# New Section"""

#Setting learning rate and iterations
lr = 0.1
iterations = 1000
initializations = 50

#Defining no. of samples and features
nsamples, nfeatures = X.shape

#Defining the sigmoid funtion
def sigmoid(x):
        return 1 / (1 + np.exp(-x))

#Defining Cost, Coefficients and self bias matrix

J = np.zeros(initializations*7).reshape(initializations, 7)
cost = np.zeros(initializations*iterations).reshape(iterations, initializations)

#Gradient Descent


for i in range(initializations):
  beta = random.sample(range(-10, 10), 5)
  bias = random.sample(range(-10, 10), 1)


  for j in range(iterations):
            
      lin_comb_train = np.dot(X_train, beta) + bias
            
      model_train = sigmoid(lin_comb_train)

      dbeta = (1 / len(Y_train)) * np.dot(X_train.T, (model_train - Y_train))
      dbias = (1 / len(Y_train)) * np.sum(model_train - Y_train)
           
      beta = beta - lr * dbeta
      bias = bias - lr * dbias

      cost[j][i] = -(1 / len(Y_train)) * ( np.dot(Y_train, np.log(model_train)) + np.dot((np.ones(len(Y_train))-Y_train), (np.log(np.ones(len(Y_train))-model_train))))

  J[i][0] = cost[iterations-1][i] #Storing the final cost, beta and bias
  J[i][1:6] = beta.T
  J[i][6] = bias

#Plot the cost function values for all initializations

import matplotlib.pyplot as plt
plt.plot(J[:,0])

print (min(J[:,0]), "is the minimum value of cost funtion amongst all random initializtions")

cost_initial = J[:, 0]
print (cost_initial)
index = np.argmin(cost_initial)
print (index)           #The index of initialization for which the cost minimum

#The descent of the cost function value

plt.plot(cost[:,index])
min(J[:,0])

J[:,0]

beta = J[index, 1:6]
bias = J[index, 6]

#Running model over test data

lin_comb_test = np.dot(X_test, beta) + bias
model_test = sigmoid(lin_comb_test)

#Converting probabilities to categories
model_test_label = [1 if i > 0.5 else 0 for i in model_test]

model_test_label[:10]   #checking output

#Creating confusion matrix

import sklearn
from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(Y_test, model_test_label)
cnf_matrix

beta    #check coefficients

bias    #Check self bias

import matplotlib.pyplot as plt
import seaborn as sns

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#Model Evaluation

print("The Accuracy is          ", sklearn.metrics.accuracy_score(Y_test, model_test_label))
print("The F1 score is          ", sklearn.metrics.f1_score(Y_test, model_test_label))
print("The balanced accuracy is ", sklearn.metrics.balanced_accuracy_score(Y_test, model_test_label))
print("The Precision score is   ", sklearn.metrics.precision_score(Y_test, model_test_label))
print("The Recall score is      ", sklearn.metrics.recall_score(Y_test, model_test_label))

#Running Model on training data

lin_comb_train = np.dot(X_train, beta) + bias
            
model_train = sigmoid(lin_comb_train)
model_train_label = [1 if i > 0.5 else 0 for i in model_train]

import sklearn
from sklearn import metrics
cnf_matrix2 = metrics.confusion_matrix(Y_train, model_train_label)
cnf_matrix2

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix2), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

#Model Evaluation on training data

print("The training Accuracy is          ", sklearn.metrics.accuracy_score(Y_train, model_train_label))
print("The traing F1 score               ", sklearn.metrics.f1_score(Y_train, model_train_label))
print("The training balanced accuracy is ", sklearn.metrics.balanced_accuracy_score(Y_train, model_train_label))
print("The training Precision score is   ", sklearn.metrics.precision_score(Y_train, model_train_label))
print("The training Recall score is      ", sklearn.metrics.recall_score(Y_train, model_train_label))

#Final Model Evaluation

print("The Accuracy is          ", sklearn.metrics.accuracy_score(Y_test, model_test_label))
print("The F1 score is          ", sklearn.metrics.f1_score(Y_test, model_test_label))
print("The balanced accuracy is ", sklearn.metrics.balanced_accuracy_score(Y_test, model_test_label))
print("The Precision score is   ", sklearn.metrics.precision_score(Y_test, model_test_label))
print("The Recall score is      ", sklearn.metrics.recall_score(Y_test, model_test_label))